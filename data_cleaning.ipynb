{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning steps:\n",
    "- replace @ handles with: hdl (chose a word with no punctuation so no extra step is needed to process, hdl = handle)\n",
    "- replace urls by: url\n",
    "- replace emoticons with corresponding emojis\n",
    "- split retweets in tweet and retweet\n",
    "\n",
    "For all of these, first check what is the prevalence and if it is worth the effort. \n",
    "\n",
    "Ideas:\n",
    "- split hashtags into words -> Carlo tackling that\n",
    "- Tweets not in english -> Daniel tackling that\n",
    "- replace contractions & spellchecking\n",
    "    - Character ngram will probably be more efficient due to the really low quality of speach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 140\n",
    "import nltk\n",
    "import re\n",
    "from IPython.display import display\n",
    "import happyfuntokenizing\n",
    "from happyfuntokenizing import TweetTokenizer\n",
    "\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholder cannot be called in this file before the all subfunctions are defined\n",
    "def clean(df):\n",
    "    \"\"\"Data cleaning steps:\n",
    "    - replace @ handles with: hdl (chose a word with no punctuation so no extra step is needed to process, hdl = handle)\n",
    "    - replace urls by: url\n",
    "    - replace emoticons with corresponding emojis\n",
    "    - split retweets in tweet and retweet\n",
    "    - remove non english tweets, done on the text only (not retweet). Needs to be applied after emoji split. \n",
    "    - hashtags splits\n",
    "    - subset by only necessary fields\n",
    "    \n",
    "    Returns None (data cleaning in place)\n",
    "    \"\"\"\n",
    "    cleanHandle(df)\n",
    "    cleanURL(df)\n",
    "    convertEmoticon(df)\n",
    "    cleanRetweets(df)\n",
    "    splitTextEmoji(df)\n",
    "    df = cleanNonEnglish(df)\n",
    "    \n",
    "    tokenize_total_text(df)\n",
    "    total_text = tokenize_total_text(df)\n",
    "    \"\"\" wordcost and maxword used from 'Generic Human' on stackoverflow question\n",
    "        http://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words\"\"\"\n",
    "    wordcost = dict((k, log((i+1)*log(len(total_text)))) for i,k in enumerate(total_text))\n",
    "    maxword = max(len(x) for x in total_text)\n",
    "    \n",
    "    run_hashtag(df, maxword, wordcost)\n",
    "    df = df.drop(['lat','lng','only_text','timeStamp'], axis=1)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loader(filename):\n",
    "    \"\"\" Load tweets from filename. Resets the index. Returns the loaded data frame\"\"\"\n",
    "    with open(filename,'r') as f:\n",
    "        df = pd.DataFrame(json.load(f))\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace @ handles with hdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanHandle(df):\n",
    "    \"\"\" Replace in-place handles with hdl keyword\n",
    "    \n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    pattern = r\"@[a-zA-Z0-9_]{1,15}\" #from http://kagan.mactane.org/blog/2009/09/22/what-characters-are-allowed-in-twitter-usernames/\n",
    "    print(\"{} handles replaced\".format(np.sum(df.text.str.contains(pattern).values)))\n",
    "    df.text = df.text.str.replace(pattern, \" hdl \")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No \"hdl\" words for confusion in the txt. Good replacement name. \n",
    "\n",
    "Best to replace handles with \"\\ hdl\\ \", so for tokenization it will be easier to identify as a word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace URLs with url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanURL(df):\n",
    "    \"\"\" Replace in-place URLs with url keyword\n",
    "    \n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    pattern = r'(?:http://|https://|www.)[^“”\"\\' ]+' # From http://stackoverflow.com/questions/7679970/python-regular-expression-nltk-website-extraction\n",
    "    print(\"{} urls replaced\".format(np.sum(df.text.str.contains(pattern).values)))\n",
    "    df.text = df.text.str.replace(pattern, \" url \")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URLs are quiet well formed and are generally at the end of tweets. No risk of engulfing in the cleaning some more text after the url.\n",
    "\n",
    "keyword url is used only 4 times in dataset, no risk of confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert emoticons to emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Based on:\n",
    "# https://slack.zendesk.com/hc/en-us/articles/202931348-Emoji-and-emoticons\n",
    "# http://unicodey.com/emoji-data/table.htm\n",
    "# http://www.unicode.org/emoji/charts/emoji-list.html\n",
    "\n",
    "def convertEmoticon(df):\n",
    "    \"\"\" Replace in-place common emoticons to emojis.\n",
    "    \n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    emoticon2emoji = {\n",
    "        r\"<3\": \"\\u2764\",\n",
    "        r\"</3\": \"\\U0001F494\",\n",
    "        r\"8\\)\": \"\\U0001F60E\",\n",
    "        r\"D:\": \"\\U0001F627\",\n",
    "        r\":'\\(\": \"\\U0001F622\",\n",
    "        r\":o\\)\": \"\\U0001F435\",\n",
    "        r\":-*\\*\": \"\\U0001F48B\",\n",
    "        r\"=-*\\)\": \"\\U0001F600\",\n",
    "        r\":-*D\": \"\\U0001F600\",\n",
    "        r\";-*\\)\": \"\\U0001F609\",\n",
    "        r\":-*>\": \"\\U0001F606\",\n",
    "        r\":-*\\|\": \"\\U0001F610\",\n",
    "        r\":-*[Oo]\": \"\\U0001F62E\",\n",
    "        r\">:-*\\(\": \"\\U0001F620\",\n",
    "        r\":-*\\)|\\(:\": \"\\U0001F603\",\n",
    "        r\":-*\\(|\\):\": \"\\U0001F61E\",\n",
    "        r\":-*[/\\\\]\": \"\\U0001F615\",\n",
    "        r\":-*[PpbB]\": \"\\U0001F61B\",\n",
    "        r\";-*[PpbB]\": \"\\U0001F61C\"\n",
    "    }\n",
    "    \n",
    "    total = 0\n",
    "    for emoticon in emoticon2emoji:\n",
    "        nreplacements = np.sum(df.text.str.contains(emoticon).values)\n",
    "        total += nreplacements\n",
    "        print(\"{:10} -> {:>5} replaced {:6} times\".format(emoticon, emoticon2emoji[emoticon], nreplacements))\n",
    "        df.text = df.text.str.replace(emoticon, emoticon2emoji[emoticon])\n",
    "    print(\"{:3} replaced {} times\".format(\"ALL\", total))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split retweets into user content and retweeted content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cannot be called before functions within are defined \n",
    "def cleanRetweets(df):\n",
    "    \"\"\" Remove from the text column the retweet column and add to seperate column \"retweet\". \n",
    "    \n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    nsplits = 0\n",
    "    nsplits += splitRetweets(df)\n",
    "    nsplits += splitQuotes(df) # Need to maintain that order\n",
    "    print(\"{} retweets processed\".format(nsplits))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitRetweets(df):\n",
    "    \"\"\" Extract retweets with the RT keyword\"\"\"\n",
    "    pattern = r\"\"\"(.*(?:\\W|^))(RT(?:\\ ?[\\\":“]|$).*) # Retweets with RT keyword\"\"\"\n",
    "    retweets = pd.DataFrame(df.text.str.extract(pattern, flags=re.X))\n",
    "    retweets.columns = [\"text\", \"retweet\"]\n",
    "    non_null_idxs = retweets.retweet.notnull()\n",
    "    df.loc[non_null_idxs,[\"text\"]] = retweets.text[non_null_idxs]\n",
    "    df[\"retweet\"] = retweets.retweet\n",
    "    return len(non_null_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def splitQuotes(df):\n",
    "    \"\"\" Extract retweets in quote format.\n",
    "    See http://support.gnip.com/articles/identifying-and-understanding-retweets.html\n",
    "    \"\"\"\n",
    "    pattern = r\"(.*?)((?:([\\\"\\'])|(?:(“))|‘)\\s*hdl.*(?(3)\\3|(?(4)”|’)))(.*)\" #Pattern to match quote, possibly nested\n",
    "    retweets = pd.DataFrame(df.text.str.extract(pattern, flags=re.X)[[0,1,4]])\n",
    "    retweets.columns = [\"text_before\", \"retweet\", \"text_after\"]\n",
    "    non_null_idxs = retweets.retweet.notnull()\n",
    "    retweets[\"text\"] = retweets.loc[non_null_idxs, [\"text_before\", \"text_after\"]].apply(lambda x: \" \".join(x), axis=1)\n",
    "    df.loc[non_null_idxs,[\"text\"]] = retweets.text[non_null_idxs].copy()\n",
    "    df[\"retweet\"] = retweets.retweet.copy()\n",
    "    return len(retweets[non_null_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Text and Emoji and create two new columns for only text and only emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Wide UCS-4 build\n",
    "    highpoints = re.compile(u'['\n",
    "        u'\\U0001F300-\\U0001F64F'\n",
    "        u'\\U0001F680-\\U0001F6FF'\n",
    "        u'\\u2600-\\u26FF\\u2700-\\u27BF]+', \n",
    "        re.UNICODE)\n",
    "except re.error:\n",
    "    # Narrow UCS-2 build\n",
    "    highpoints = re.compile(u'('\n",
    "        u'\\ud83c[\\udf00-\\udfff]|'\n",
    "        u'\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|'\n",
    "        u'[\\u2600-\\u26FF\\u2700-\\u27BF])+', \n",
    "        re.UNICODE)\n",
    "# Functions to check whether there's an emoji in the text, return 1 if true, 0 if false\n",
    "def is_emoji(text):\n",
    "    if highpoints.search(text):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def splitTextEmoji(df):\n",
    "    tok = happyfuntokenizing.TweetTokenizer(preserve_case=False)\n",
    "    def emojiExtract(sent):\n",
    "        return [word for word in tok.tokenize(sent) if is_emoji(word) == 1]\n",
    "\n",
    "    def textExtract(sent):\n",
    "        return ''.join([word for word in sent if is_emoji(word) == 0])\n",
    "\n",
    "    def addEmoji(df):\n",
    "        df['only_emoji'] = [emojiExtract(word) for word in df.text]\n",
    "\n",
    "    def addText(df):\n",
    "        df['only_text'] = [textExtract(word) for word in df.text]\n",
    "    \n",
    "    addText(df)\n",
    "    addEmoji(df)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Functions to clean non-english columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = string.punctuation\n",
    "ex = ['“', '—', '’', ' ️', '️', '...', '”', '…', ' , , ,', '?', ' ', ' ⃣', '∞', '🆒']\n",
    "for pun in [word for word in ex if word not in punctuation]:\n",
    "    punctuation += pun\n",
    "    \n",
    "def isEnglish(list):\n",
    "    try:\n",
    "        [word.encode('ascii') for word in list if word not in punctuation]\n",
    "    except Exception:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "def cleanNonEnglish(df):\n",
    "    \"\"\" \n",
    "    \n",
    "    Needs to be applied after emoji splitting as emojis are considered non-english\n",
    "    \"\"\"\n",
    "    text_list = df['only_text'].values\n",
    "    english_Boolean = [isEnglish(sent) for sent in text_list]\n",
    "    print(\"{} number of tweets are not English\".format(len(english_Boolean) - sum(english_Boolean)))\n",
    "    return df[english_Boolean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtag Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_total_text(df):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    total_text = df.text.apply(lambda x: tokenizer.tokenize(x))\n",
    "    \n",
    "    stop_words = nltk.corpus.stopwords.words('english') + [\"http\", 'hdl', 'url'] \n",
    "    punctuation_words = list(set(string.punctuation)) + [\":\", \":/\"]\n",
    "\n",
    "    total_text = list(total_text)\n",
    "    total_text = [word for _list in total_text for word in _list if word not in stop_words and word not in punctuation_words]\n",
    "    \n",
    "    return total_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_hashtag(df, maxword, wordcost):\n",
    "    def addHashTag(df):\n",
    "        df['only_HashTag'] = [re.findall(r\"(#\\w+)\", word) for word in df.text]\n",
    "\n",
    "    def is_hashtag(text):\n",
    "        if re.search(r\"(#\\w+)\", text):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def textEmojiExtract(sent):\n",
    "        tok = happyfuntokenizing.TweetTokenizer(preserve_case=False)\n",
    "        return ' '.join([word for word in tok.tokenize(sent) if is_hashtag(word) == 0])\n",
    "\n",
    "    def textHashtagSplit(sent):\n",
    "        tok = happyfuntokenizing.TweetTokenizer(preserve_case=True)\n",
    "        return ' '.join([word if is_hashtag(word) == 0 else apply_hashtag_text(word, maxword, wordcost) for word in tok.tokenize(sent)])\n",
    "\n",
    "    def noHashtagText(df, wordcost):\n",
    "        df['only_text_splithashtag'] = [textHashtagSplit(word) for word in df.only_text]\n",
    "    #     df['text_splithashtag'] = [textHashtagSplit(word) for word in df.text]\n",
    "        return\n",
    "\n",
    "    def infer_spaces(s, maxword, wordcost):\n",
    "        \"\"\" Code used from 'Generic Human' on stackoverflow question\n",
    "        http://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words\"\"\"\n",
    "\n",
    "        # Build a cost dictionary, assuming Zipf's law and cost = -math.log(probability).\n",
    "\n",
    "        # Find the best match for the i first characters, assuming cost has\n",
    "        # been built for the i-1 first characters.\n",
    "        # Returns a pair (match_cost, match_length).\n",
    "        def best_match(i):\n",
    "            candidates = enumerate(reversed(cost[max(0, i-maxword):i]))\n",
    "            return min((c + wordcost.get(s[i-k-1:i], 9e999), k+1) for k,c in candidates)\n",
    "\n",
    "        # Build the cost array.\n",
    "        cost = [0]\n",
    "        for i in range(1,len(s)+1):\n",
    "            c,k = best_match(i)\n",
    "            cost.append(c)\n",
    "\n",
    "        # Backtrack to recover the minimal-cost string.\n",
    "        out = []\n",
    "        i = len(s)\n",
    "        while i>0:\n",
    "            c,k = best_match(i)\n",
    "            assert c == cost[i]\n",
    "            out.append(s[i-k:i])\n",
    "            i -= k\n",
    "\n",
    "        return list(reversed(out))\n",
    "\n",
    "    def apply_hashtag_split(_hashtag, maxword, wordcost):\n",
    "        # for each hashtag in list of hashtags, split on # and take second item\n",
    "        item = [item.split('#')[1] for item in _hashtag if len(item) > 0]\n",
    "\n",
    "        # regex pattern\n",
    "        # (1) split on two caps or more, and only keep caps\n",
    "        # (2) split on exactly one cap or more, and keep trailing letters\n",
    "        pattern = r'([A-Z]{2,})|([A-Z]{1}[a-z]*)'\n",
    "\n",
    "        # loop through each item in list of hashtags\n",
    "        final_hashtags = []\n",
    "        for word in item:\n",
    "\n",
    "            # if len of word is 0, then there is no hashtag\n",
    "            if len(word) == 0:\n",
    "                final_hashtags.append(\"empty_hashtag\")\n",
    "\n",
    "            # use (1)) regex: funciton lowercase, as \"Treatlowercase\" can be treated as lowercase\n",
    "            elif word[0].isupper() and word[1:].islower():\n",
    "                final_hashtags.append(infer_spaces(word.lower(), maxword, wordcost))\n",
    "\n",
    "            # use (1) regex: funciton lowercase\n",
    "            elif word.islower():\n",
    "                final_hashtags.append(infer_spaces(word, maxword, wordcost))\n",
    "\n",
    "            # use (2) regex: customized uppercase\n",
    "            else:\n",
    "                final_hashtags.append(list(filter(None, re.split(pattern, word))))\n",
    "        return final_hashtags\n",
    "\n",
    "    def apply_hashtag_text(_hashtag, maxword, wordcost):\n",
    "        word = _hashtag.split('#')[1] \n",
    "\n",
    "        # regex pattern\n",
    "        # (1) split on two caps or more, and only keep caps\n",
    "        # (2) split on exactly one cap or more, and keep trailing letters\n",
    "        pattern = r'([A-Z]{2,})|([A-Z]{1}[a-z]*)'\n",
    "\n",
    "        # if len of word is 0, then there is no hashtag\n",
    "        if len(word) == 0:\n",
    "            final_hashtags = \"\"\n",
    "\n",
    "        # use (1)) regex: funciton lowercase, as \"Treatlowercase\" can be treated as lowercase\n",
    "        elif word[0].isupper() and word[1:].islower():\n",
    "            final_hashtags =  infer_spaces(word.lower(), maxword, wordcost)\n",
    "\n",
    "        # use (1) regex: funciton lowercase\n",
    "        elif word.islower():\n",
    "            final_hashtags = infer_spaces(word, maxword, wordcost)\n",
    "\n",
    "        # use (2) regex: customized uppercase\n",
    "        else:\n",
    "            final_hashtags = list(filter(None, re.split(pattern, word)))\n",
    "\n",
    "        joined_hashtags = ' '.join(final_hashtags)\n",
    "        return joined_hashtags\n",
    "    \n",
    "    addHashTag(df)\n",
    "    noHashtagText(df, wordcost)\n",
    "    df[\"split_hashtag\"] = df.only_HashTag.apply(apply_hashtag_split, maxword=maxword, wordcost=wordcost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save clean file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73386 handles replaced\n",
      "46844 urls replaced\n",
      ":-*\\|      ->     😐 replaced     15 times\n",
      ":-*>       ->     😆 replaced      0 times\n",
      ":-*[Oo]    ->     😮 replaced     73 times\n",
      "D:         ->     😧 replaced     39 times\n",
      "<3         ->     ❤ replaced      0 times\n",
      "=-*\\)      ->     😀 replaced     40 times\n",
      ":o\\)       ->     🐵 replaced      0 times\n",
      "</3        ->     💔 replaced      0 times\n",
      ":-*[PpbB]  ->     😛 replaced    223 times\n",
      ":'\\(       ->     😢 replaced     67 times\n",
      ":-*D       ->     😀 replaced    314 times\n",
      ":-*\\*      ->     💋 replaced     62 times\n",
      ":-*\\)|\\(:  ->     😃 replaced   3138 times\n",
      ":-*[/\\\\]   ->     😕 replaced    422 times\n",
      ":-*\\(|\\):  ->     😞 replaced   1163 times\n",
      ";-*\\)      ->     😉 replaced    707 times\n",
      "8\\)        ->     😎 replaced     16 times\n",
      ";-*[PpbB]  ->     😜 replaced     68 times\n",
      ">:-*\\(     ->     😠 replaced      0 times\n",
      "ALL replaced 6347 times\n",
      "208809 retweets processed\n",
      "4870 number of tweets are not English"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlo_liquido/anaconda/lib/python3.4/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/carlo_liquido/anaconda/lib/python3.4/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "293672 handles replaced\n",
      "187643 urls replaced\n",
      ":-*\\|      ->     😐 replaced     70 times\n",
      ":-*>       ->     😆 replaced      0 times\n",
      ":-*[Oo]    ->     😮 replaced    295 times\n",
      "D:         ->     😧 replaced    167 times\n",
      "<3         ->     ❤ replaced      0 times\n",
      "=-*\\)      ->     😀 replaced    175 times\n",
      ":o\\)       ->     🐵 replaced      0 times\n",
      "</3        ->     💔 replaced      0 times\n",
      ":-*[PpbB]  ->     😛 replaced    960 times\n",
      ":'\\(       ->     😢 replaced    263 times\n",
      ":-*D       ->     😀 replaced   1218 times\n",
      ":-*\\*      ->     💋 replaced    238 times\n",
      ":-*\\)|\\(:  ->     😃 replaced  12562 times\n",
      ":-*[/\\\\]   ->     😕 replaced   1571 times\n",
      ":-*\\(|\\):  ->     😞 replaced   4550 times\n",
      ";-*\\)      ->     😉 replaced   2689 times\n",
      "8\\)        ->     😎 replaced     81 times\n",
      ";-*[PpbB]  ->     😜 replaced    263 times\n",
      ">:-*\\(     ->     😠 replaced      0 times\n",
      "ALL replaced 25102 times\n",
      "834937 retweets processed\n",
      "19803 number of tweets are not English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlo_liquido/anaconda/lib/python3.4/site-packages/ipykernel/__main__.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    clean_test_tweets_df = loader('./data/tweets_test.json')\n",
    "    clean_test_tweets_df = clean(clean_test_tweets_df)\n",
    "    clean_test_tweets_df.to_json('./data/tweets_test_clean.json', force_ascii=False)\n",
    "    \n",
    "    clean_training_tweets_df = loader('./data/tweets_training.json')\n",
    "    clean_training_tweets_df = clean(clean_training_tweets_df)\n",
    "    clean_training_tweets_df.to_json('./data/tweets_training_clean.json', force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
