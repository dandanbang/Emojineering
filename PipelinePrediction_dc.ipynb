{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## import all necessary packages\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import *\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import happyfuntokenizing\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "#from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# NLTK\n",
    "from nltk import word_tokenize, wordpunct_tokenize, pos_tag\n",
    "from nltk.wsd import lesk\n",
    "import nltk, string\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# abydos\n",
    "from abydos.qgram import QGrams\n",
    "from abydos.phonetic import double_metaphone, soundex\n",
    "from abydos.clustering import skeleton_key, omission_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data/tweets_training_clean_preprocessing_v2.json','r') as f:\n",
    "    tweet_df = DataFrame(json.load(f))\n",
    "with open('./data/tweets_test_clean_preprocessingv2.json','r') as f:\n",
    "    test_df = DataFrame(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweet_random = tweet_df.sample(n=338134 ,random_state=666,axis=0)\n",
    "tweet_random = tweet_random[['category_numeric', 'only_emoji', 'only_text_splithashtag', 'retweet', 'split_hashtag', 'text']]\n",
    "test_df = test_df[['category_numeric', 'only_emoji', 'only_text_splithashtag', 'retweet', 'split_hashtag', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_random = tweet_random[tweet_random[\"category_numeric\"] != 6] \n",
    "test_df = test_df[test_df[\"category_numeric\"] != 6] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set testing to True to keep a consistent seed for functions that take a seed\n",
    "TESTING = True\n",
    "def seed():\n",
    "    if TESTING:\n",
    "        return 256\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# WordNet Lemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Function to label emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Wide UCS-4 build\n",
    "    highpoints = re.compile(u'['\n",
    "        u'\\U0001F300-\\U0001F64F'\n",
    "        u'\\U0001F680-\\U0001F6FF'\n",
    "        u'\\u2600-\\u26FF\\u2700-\\u27BF]+', \n",
    "        re.UNICODE)\n",
    "except re.error:\n",
    "    # Narrow UCS-2 build\n",
    "    highpoints = re.compile(u'('\n",
    "        u'\\ud83c[\\udf00-\\udfff]|'\n",
    "        u'\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|'\n",
    "        u'[\\u2600-\\u26FF\\u2700-\\u27BF])+', \n",
    "        re.UNICODE)\n",
    "\n",
    "def is_emoji(text):\n",
    "    if highpoints.search(text):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Function to split training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#splitting the training data into \n",
    "def create_training_sets (training_data):\n",
    "    # Create the features sets.  Call the function that was passed in.\n",
    "    # For names data, key is the name, and value is the gender\n",
    "\n",
    "    # Divided training and testing in thirds.  Could divide in other proportions instead.\n",
    "    third = int(float(len(training_data)) / 3.0)    \n",
    "    train_set, test_set = training_data[0:third*2], training_data[third*2:]\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Function to transform pandas for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful Transformer from http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "# This pulls a single column from a supplied pandas dataframe for classification.\n",
    "class ColumnExtractor(TransformerMixin):\n",
    "    def __init__(self, columns=[]):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X[self.columns]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Feature Building Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def otherFeature(sentence, retweet, hashtag):\n",
    "    blob = TextBlob(sentence)\n",
    "    sentimentSum = 0\n",
    "    tweet = 0\n",
    "    for sentence in blob.sentences:\n",
    "        sentimentSum += sentence.sentiment.polarity\n",
    "    if retweet == None:\n",
    "        tweet =  0\n",
    "    else:\n",
    "        tweet = 1\n",
    "    count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "    a_punct = count(sentence, string.punctuation)\n",
    "    dict_ = {'sentiment' : sentimentSum,\n",
    "    'textLength' : len(sentence),\n",
    "    'isRetweet': tweet,\n",
    "    'numPunct': a_punct,\n",
    "    'hashTag': len(hashtag)}\n",
    "    return dict_\n",
    "\n",
    "def sentimentScore(sentence):\n",
    "    blob = TextBlob(sentence)\n",
    "    sentimentSum = 0\n",
    "    for sentence in blob.sentences:\n",
    "        sentimentSum += sentence.sentiment.polarity\n",
    "    return sentimentSum\n",
    "\n",
    "def textLength(text):\n",
    "    return len(text)\n",
    "\n",
    "def isRetweet(text):\n",
    "    if text == None:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def numPunct(text):\n",
    "    count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "    a_punct = count(text, string.punctuation)\n",
    "    return a_punct\n",
    "\n",
    "def leskize(word_pos_str):\n",
    "    \"\"\"Returns the most probable WordNet lemmas for each term in a string by applying the lesk aglorithm\n",
    "    to each term, given its tagged POS and the string context\n",
    "    \n",
    "    Arguments:\n",
    "    word_pos_str -- str consisting of \"word POS word POS word POS ...\"\n",
    "    \n",
    "    Returns:\n",
    "    list\n",
    "    \"\"\"\n",
    "    wn_pos = {'VERB': 'v', 'NOUN': 'n', 'ADV': 'r', 'ADJ': 'a'}\n",
    "    \n",
    "    ss_list = []\n",
    "\n",
    "    word_pos_pairs = word_pos_str.split()\n",
    "    words = word_pos_pairs[::2]\n",
    "    for i in range(0, len(word_pos_pairs), 2):\n",
    "        if word_pos_pairs[i+1] in wn_pos:\n",
    "            ss = lesk(words, word_pos_pairs[i], wn_pos[word_pos_pairs[i+1]])\n",
    "            if not ss:\n",
    "                ss = lesk(words, word_pos_pairs[i])\n",
    "            if ss:\n",
    "                ss_list.append(ss)\n",
    "\n",
    "    return ss_list\n",
    "\n",
    "def build_features(df):\n",
    "    \"\"\"Do basic processing of the input text and generate features based on it in different columns\n",
    "\n",
    "    Arguments:\n",
    "    df -- DataFrame (the pandas dataframe with Category & Text columns already defined)\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "#     # Part of Speech in the form \"word POS word POS word POS ...\"\n",
    "#     df['pos'] = df.only_text_splithashtag.apply(lambda sent: ' '.join([' '.join([wnl.lemmatize(word), tag]) for word, tag\n",
    "#                                                               in pos_tag(wordpunct_tokenize(sent),\n",
    "#                                                                          tagset='universal') if\n",
    "#                                                               tag[0] not in string.punctuation]))\n",
    "\n",
    "#     # A list of most probable lemma synsets (not used directly, but useful)\n",
    "#     df['synsets'] = df.pos.apply(leskize)\n",
    "\n",
    "#     # The definitions of each word, concatenated\n",
    "#     df['definition'] = df.synsets.apply(lambda sss: ' '.join([ss.definition() for ss in sss]))\n",
    "\n",
    "    # q-grams (generated from Chris' abydos package, which performed a little better than TfidfVectorizer)\n",
    "    # These are also known as k-grams, shingles, k-mers, and (character-wise) n-grams\n",
    "    df['other_features'] = df.apply(lambda x: otherFeature(x['only_text_splithashtag'], x['retweet'], x['split_hashtag']), axis=1)\n",
    "    \n",
    "    \n",
    "    df['qgrams'] = df.only_text_splithashtag.apply(lambda s: dict(QGrams(s, 4, start_stop='') + QGrams(s, 5, start_stop='')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Pipeline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok = happyfuntokenizing.TweetTokenizer(preserve_case=False)\n",
    "# Logistic classifier, using WordNet-lemmatized unigrams & bigrams as features\n",
    "log_lem12_pipeline = Pipeline([\n",
    "            ('tfidf_lemmatized', Pipeline([\n",
    "                    ('extract', ColumnExtractor('tokenized')),\n",
    "                    ('vectorize', CountVectorizer(ngram_range=(1, 2),\n",
    "                                                  lowercase=True, tokenizer=tok.tokenize)),\n",
    "            ])),\n",
    "            ('classifier', LogisticRegression(random_state=seed()))])\n",
    "\n",
    "# lsvc_lem12_pipeline = Pipeline([\n",
    "#             ('tfidf_lemmatized', Pipeline([\n",
    "#                     ('extract', ColumnExtractor('tokenized')),\n",
    "#                     ('vectorize', TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, norm='l2',\n",
    "#                                                   lowercase=True, tokenizer=tok.tokenize)),\n",
    "#                     # LSA is cool, in theory, but didn't work here, except with huge values & overfitting\n",
    "#                     #('lsa', TruncatedSVD(n_components=2100, algorithm='arpack', random_state=seed())),\n",
    "#             ])),\n",
    "#             ('classifier', LinesarSVC(loss='hinge', C=1, random_state=seed()))])\n",
    "\n",
    "# SVM with a linear kernel, using unigrams & bigrams of sentences passed through soundex and through\n",
    "# double metaphone as features\n",
    "\n",
    "lsvc_sdx_pipeline = Pipeline([\n",
    "            ('features', FeatureUnion([\n",
    "                ('definition', Pipeline([\n",
    "                    ('extract', ColumnExtractor('only_text_splithashtag')),\n",
    "                    ('vectorize', TfidfVectorizer(ngram_range=(1, 3), sublinear_tf=True, norm='l2',\n",
    "                                                  lowercase=False, stop_words=\"english\")),\n",
    "                ])),\n",
    "                ('definition', Pipeline([\n",
    "                    ('extract', ColumnExtractor('only_text_splithashtag')),\n",
    "                    ('vectorize', CountVectorizer(ngram_range=(1, 2),lowercase=True)),\n",
    "                ])),\n",
    "                ('definition', Pipeline([\n",
    "                        ('extract', ColumnExtractor('qgrams')),\n",
    "                        ('vectorize', DictVectorizer()),\n",
    "                ])),\n",
    "                ('definition', Pipeline([\n",
    "                        ('extract', ColumnExtractor('other_features')),\n",
    "                         ('vectorize', DictVectorizer()),\n",
    "                ])),\n",
    "                    ])),\n",
    "                ('classifier', LogisticRegression(C=0.001, random_state=seed()))\n",
    "#                ('classifier', XGBClassifier(max_depth=8,n_estimators=128,))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6. Building the Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "build_features(tweet_random)\n",
    "build_features(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##7. Splitting the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set, test_set = create_training_sets(tweet_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##8. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "TESTING = False\n",
    "model = lsvc_sdx_pipeline.fit(train_set, train_set['category_numeric'])\n",
    "prediction = model.predict(test_df)\n",
    "score = accuracy_score(test_df['category_numeric'], prediction)\n",
    "print(score)\n",
    "\n",
    "sec = time.time() - start_time\n",
    "hours, remainder = divmod(sec, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print('time to completion: %02d:%02d:%02d' % (hours, minutes, seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 84701)]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "f = FreqDist(prediction) \n",
    "f.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812303501572\n",
      "time to completion: 00:59:41\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "start_time = time.time()\n",
    "\n",
    "TESTING = False\n",
    "model = lsvc_sdx_pipeline.fit(train_set, train_set['category_numeric'])\n",
    "prediction = model.predict(test_set)\n",
    "score = accuracy_score(test_set['category_numeric'], prediction)\n",
    "print(score)\n",
    "\n",
    "sec = time.time() - start_time\n",
    "hours, remainder = divmod(sec, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print('time to completion: %02d:%02d:%02d' % (hours, minutes, seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9. Saving The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('logisticRegression.pickle', 'wb') as fin:\n",
    "    pickle.dump(model, fin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
