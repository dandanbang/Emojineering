{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Word2Vec Demo##\n",
    "From https://github.com/nltk/nltk/blob/develop/nltk/test/gensim.doctest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to get gensim, to to https://radimrehurek.com/gensim/\n",
    "# OR run this on your command line: easy_install -U gensim \n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.data import find\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To get the model file needed, do the following one time only:\n",
    "#one time only: Run download; view the UI that pops up; switch to the models tab, and download the word2vec_sample model\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.Word2Vec.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-99f67e885465>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2vec_sample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "for doc in word2vec_sample:\n",
    "    words = filter(lambda x: x in model.vocab, doc.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF FOR REFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pruned the model to only include the most common words (~44k words).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43981"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word is represented in the space of 300 dimensions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model['university'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the top n words that are similar to a target word is simple. The result is the list of n words with the score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universities', 0.7003918290138245),\n",
       " ('faculty', 0.6780907511711121),\n",
       " ('undergraduate', 0.6587095260620117),\n",
       " ('campus', 0.6434987783432007),\n",
       " ('college', 0.6385269165039062),\n",
       " ('academic', 0.6317198276519775),\n",
       " ('professors', 0.6298646926879883),\n",
       " ('undergraduates', 0.6149813532829285),\n",
       " ('University', 0.6139305233955383),\n",
       " ('student', 0.6005401611328125)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['university'], topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a word that is not in a list is also supported in the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match('breakfast cereal dinner lunch'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mikolov et al. (2013) figured out the following famous exampe:  word embedding captures much of syntactic and semantic regularities. For example,\n",
    "the vector 'King - Man + Woman' is close to 'Queen' and 'Germany - Berlin + Paris' is close to 'France'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman','king'], negative=['man'], topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('win', 0.6727384924888611),\n",
       " ('victory', 0.5993870496749878),\n",
       " ('wins', 0.5796988606452942),\n",
       " ('victories', 0.5518628358840942),\n",
       " ('winning', 0.5479387044906616),\n",
       " ('triumphs', 0.5340350866317749),\n",
       " ('clinch', 0.5326650142669678),\n",
       " ('victorious', 0.5290749073028564),\n",
       " ('faces', 0.5236936807632446),\n",
       " ('defeated', 0.5202665328979492)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"face\", \"person\", 'triumph', 'won'], topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('France', 0.7884091138839722)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['Paris','Germany'], negative=['Berlin'], topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chancellor', 0.6200418472290039),\n",
       " ('dean', 0.6120452880859375),\n",
       " ('President', 0.591903805732727),\n",
       " ('faculty', 0.5726973414421082),\n",
       " ('rector', 0.5606599450111389),\n",
       " ('presidents', 0.5546602606773376),\n",
       " ('Provost', 0.5418164730072021),\n",
       " ('regents', 0.5399488210678101),\n",
       " ('professors', 0.5367733240127563),\n",
       " ('universities', 0.5157524347305298),\n",
       " ('campus', 0.5094808340072632),\n",
       " ('student', 0.5033937692642212),\n",
       " ('academic', 0.5031865835189819),\n",
       " ('institute', 0.5005171895027161),\n",
       " ('undergraduate', 0.48198601603507996),\n",
       " ('Professors', 0.47340402007102966),\n",
       " ('professor', 0.47276201844215393),\n",
       " ('Faculty', 0.47209471464157104),\n",
       " ('chairman', 0.4699815511703491),\n",
       " ('professorship', 0.467648446559906),\n",
       " ('presidency', 0.46344324946403503),\n",
       " ('University', 0.45916348695755005),\n",
       " ('campuses', 0.45756882429122925),\n",
       " ('college', 0.45753854513168335),\n",
       " ('trustees', 0.45137834548950195),\n",
       " ('Chancellor', 0.4487611949443817),\n",
       " ('undergraduates', 0.4440937042236328),\n",
       " ('institution', 0.4374503493309021),\n",
       " ('endowment', 0.4351673722267151),\n",
       " ('Trustees', 0.433601975440979)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['president', 'university'], topn=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can train your own models.  Here is an example using NLTK corpora.  This will be an exercise in seeing how different corpora yield different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_model = gensim.models.Word2Vec(brown.sents())\n",
    "\n",
    "# It might take some time to train the model. So, after it is trained, it can be saved as follows:\n",
    "\n",
    "brown_model.save('brown.embedding')\n",
    "new_model = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Cardinals', 0.9014449715614319),\n",
       " ('upheld', 0.9003393650054932),\n",
       " ('Larson', 0.8979992866516113),\n",
       " ('Corp.', 0.8945410251617432),\n",
       " ('commissioner', 0.8785369992256165),\n",
       " ('Hengesbach', 0.877533495426178),\n",
       " ('Kong', 0.875813901424408),\n",
       " ('resignation', 0.8756513595581055),\n",
       " ('Grant', 0.8734716773033142),\n",
       " ('Football', 0.8717625141143799)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.most_similar('president')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# START EMOJINEERING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_scraped_txt(txt):\n",
    "    with open(\"emoji_webscraped.txt\") as f_in:\n",
    "        titles = []\n",
    "        descriptions = []\n",
    "        annotations = []\n",
    "        for line in f_in:\n",
    "            line = line.strip()\n",
    "            temp = line.split(\", \")\n",
    "            titles.append(temp[0])\n",
    "            descriptions.append(temp[1])\n",
    "            annotations.append(temp[2:len(temp)])\n",
    "        return titles, descriptions, annotations\n",
    "\n",
    "titles, descriptions, annotations = convert_scraped_txt(\"emoji_webscraped.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1282\n",
      "1282\n",
      "1282\n"
     ]
    }
   ],
   "source": [
    "print(len(titles))\n",
    "print(len(descriptions))\n",
    "print(len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {'titles' : (titles),\n",
    "     'annotations' : (annotations),\n",
    "     'descriptions': (descriptions)}\n",
    "df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[face, grin, person]</td>\n",
       "      <td>grinning face</td>\n",
       "      <td>U+1F600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[eye, face, grin, person, smile]</td>\n",
       "      <td>grinning face with smiling eyes</td>\n",
       "      <td>U+1F601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[face, joy, person, tear]</td>\n",
       "      <td>face with tears of joy</td>\n",
       "      <td>U+1F602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[face, mouth, open, person, smile]</td>\n",
       "      <td>smiling face with open mouth</td>\n",
       "      <td>U+1F603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[eye, face, mouth, open, person, smile]</td>\n",
       "      <td>smiling face with open mouth and smiling eyes</td>\n",
       "      <td>U+1F604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               annotations  \\\n",
       "0                     [face, grin, person]   \n",
       "1         [eye, face, grin, person, smile]   \n",
       "2                [face, joy, person, tear]   \n",
       "3       [face, mouth, open, person, smile]   \n",
       "4  [eye, face, mouth, open, person, smile]   \n",
       "\n",
       "                                    descriptions   titles  \n",
       "0                                  grinning face  U+1F600  \n",
       "1                grinning face with smiling eyes  U+1F601  \n",
       "2                         face with tears of joy  U+1F602  \n",
       "3                   smiling face with open mouth  U+1F603  \n",
       "4  smiling face with open mouth and smiling eyes  U+1F604  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count vectorizer on annoations descriptions\n",
    "clustering with binary data, possibly asocaition rules\n",
    "tfidf\n",
    "feature vector\n",
    "k_means on either full vector (or on lower dimensional space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n",
      "(253, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[face, grin, person]</td>\n",
       "      <td>grinning face</td>\n",
       "      <td>U+1F600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[eye, face, grin, person, smile]</td>\n",
       "      <td>grinning face with smiling eyes</td>\n",
       "      <td>U+1F601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[face, joy, person, tear]</td>\n",
       "      <td>face with tears of joy</td>\n",
       "      <td>U+1F602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[face, mouth, open, person, smile]</td>\n",
       "      <td>smiling face with open mouth</td>\n",
       "      <td>U+1F603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[eye, face, mouth, open, person, smile]</td>\n",
       "      <td>smiling face with open mouth and smiling eyes</td>\n",
       "      <td>U+1F604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               annotations  \\\n",
       "0                     [face, grin, person]   \n",
       "1         [eye, face, grin, person, smile]   \n",
       "2                [face, joy, person, tear]   \n",
       "3       [face, mouth, open, person, smile]   \n",
       "4  [eye, face, mouth, open, person, smile]   \n",
       "\n",
       "                                    descriptions   titles  \n",
       "0                                  grinning face  U+1F600  \n",
       "1                grinning face with smiling eyes  U+1F601  \n",
       "2                         face with tears of joy  U+1F602  \n",
       "3                   smiling face with open mouth  U+1F603  \n",
       "4  smiling face with open mouth and smiling eyes  U+1F604  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_titles = [list(item) for item in list(df.annotations)]\n",
    "index_face_person = [index for index,value in enumerate(list_titles) if 'face' in value or 'person' in value]\n",
    "print(len(index_face_person))\n",
    "df_face_person = df.iloc[index_face_person]\n",
    "print(df_face_person.shape)\n",
    "df_face_person.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_words = [word for item in list(df.annotations) for word in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('object', 345),\n",
       " ('flag', 268),\n",
       " ('other', 258),\n",
       " ('symbol', 244),\n",
       " ('nature', 236),\n",
       " ('person', 214),\n",
       " ('place', 179),\n",
       " ('face', 121),\n",
       " ('travel', 95),\n",
       " ('office', 85),\n",
       " ('animal', 84),\n",
       " ('sign', 64),\n",
       " ('word', 62),\n",
       " ('food', 57),\n",
       " ('time', 56),\n",
       " ('entertainment', 54),\n",
       " ('weather', 49),\n",
       " ('vehicle', 47),\n",
       " ('activity', 44),\n",
       " ('plant', 40),\n",
       " ('arrow', 39),\n",
       " ('sound', 38),\n",
       " ('sport', 37),\n",
       " ('body', 33),\n",
       " ('emotion', 32),\n",
       " ('communication', 31),\n",
       " ('clock', 29),\n",
       " ('hand', 29),\n",
       " ('zodiac', 26),\n",
       " ('tool', 26),\n",
       " ('clothing', 25),\n",
       " ('building', 23),\n",
       " ('island', 23),\n",
       " ('geometric', 22),\n",
       " ('japanese', 22),\n",
       " ('celebration', 22),\n",
       " ('heart', 22),\n",
       " ('space', 22),\n",
       " ('game', 20),\n",
       " ('mark', 18),\n",
       " ('smile', 18),\n",
       " ('ball', 17),\n",
       " ('eye', 17),\n",
       " ('religion', 15),\n",
       " ('fairy tale', 15),\n",
       " ('sweet', 14),\n",
       " ('moon', 14),\n",
       " ('prohibited', 14),\n",
       " ('no', 14),\n",
       " ('not', 14)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(list_words).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_word2vec(_list):\n",
    "    single_words = [model.most_similar(positive=item, topn = 1)  for item in _list]\n",
    "    return single_words\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'savouring' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-aa2309cdce97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-79-bd32f2e352fc>\u001b[0m in \u001b[0;36mlist_word2vec\u001b[0;34m(_list)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlist_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msingle_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msingle_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-bd32f2e352fc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlist_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msingle_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msingle_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/carlo_liquido/anaconda/lib/python3.4/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab)\u001b[0m\n\u001b[1;32m   1172\u001b[0m                 \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute similarity with no input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'savouring' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "list_word2vec(df.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
