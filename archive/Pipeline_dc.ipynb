{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%autopx disabled\n"
     ]
    }
   ],
   "source": [
    "from IPython.parallel import Client\n",
    "rc = Client()\n",
    "rc.ids\n",
    "dview = rc[:]\n",
    "%autopx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## import all necessary packages\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import *\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import happyfuntokenizing\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "#from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# NLTK\n",
    "from nltk import word_tokenize, wordpunct_tokenize, pos_tag\n",
    "from nltk.wsd import lesk\n",
    "import nltk, string\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# abydos\n",
    "from abydos.qgram import QGrams\n",
    "from abydos.phonetic import double_metaphone, soundex\n",
    "from abydos.clustering import skeleton_key, omission_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data/tweets_training_clean.json','r') as f:\n",
    "    tweet_df = DataFrame(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>only_emoji</th>\n",
       "      <th>only_text_splithashtag</th>\n",
       "      <th>retweet</th>\n",
       "      <th>split_hashtag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>301718</th>\n",
       "      <td>[]</td>\n",
       "      <td>777 - we became hella closer so quick . You ba...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>777- we became hella closer so quick. You bae ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104653</th>\n",
       "      <td>[]</td>\n",
       "      <td>I really want chicken mcnuggets . this is not ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>I really want chicken mcnuggets. this is not g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35389</th>\n",
       "      <td>[]</td>\n",
       "      <td>What happens when Caltrain fails : 8 coworkers...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>What happens when Caltrain fails: 8 coworkers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785592</th>\n",
       "      <td>[]</td>\n",
       "      <td>Just landed . Back to the 2nd home . ( @ San F...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>Just landed. Back to the 2nd home. (@ San Fran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733640</th>\n",
       "      <td>[]</td>\n",
       "      <td>BCRF @ Casa de Johnson url</td>\n",
       "      <td>None</td>\n",
       "      <td>[[BCRF]]</td>\n",
       "      <td>#BCRF @ Casa de Johnson  url</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       only_emoji                             only_text_splithashtag retweet  \\\n",
       "301718         []  777 - we became hella closer so quick . You ba...    None   \n",
       "104653         []  I really want chicken mcnuggets . this is not ...    None   \n",
       "35389          []  What happens when Caltrain fails : 8 coworkers...    None   \n",
       "785592         []  Just landed . Back to the 2nd home . ( @ San F...    None   \n",
       "733640         []                         BCRF @ Casa de Johnson url    None   \n",
       "\n",
       "       split_hashtag                                               text  \n",
       "301718            []  777- we became hella closer so quick. You bae ...  \n",
       "104653            []  I really want chicken mcnuggets. this is not g...  \n",
       "35389             []  What happens when Caltrain fails: 8 coworkers ...  \n",
       "785592            []  Just landed. Back to the 2nd home. (@ San Fran...  \n",
       "733640      [[BCRF]]                      #BCRF @ Casa de Johnson  url   "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_random = tweet_df.sample(n=500000,random_state=666,axis=0)\n",
    "tweet_random = tweet_random[['only_emoji', 'only_text_splithashtag', 'retweet', 'split_hashtag', 'text']]\n",
    "tweet_random.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set testing to True to keep a consistent seed for functions that take a seed\n",
    "TESTING = True\n",
    "def seed():\n",
    "    if TESTING:\n",
    "        return 256\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# WordNet Lemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Function to label emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Wide UCS-4 build\n",
    "    highpoints = re.compile(u'['\n",
    "        u'\\U0001F300-\\U0001F64F'\n",
    "        u'\\U0001F680-\\U0001F6FF'\n",
    "        u'\\u2600-\\u26FF\\u2700-\\u27BF]+', \n",
    "        re.UNICODE)\n",
    "except re.error:\n",
    "    # Narrow UCS-2 build\n",
    "    highpoints = re.compile(u'('\n",
    "        u'\\ud83c[\\udf00-\\udfff]|'\n",
    "        u'\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|'\n",
    "        u'[\\u2600-\\u26FF\\u2700-\\u27BF])+', \n",
    "        re.UNICODE)\n",
    "\n",
    "def is_emoji(text):\n",
    "    if highpoints.search(text):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Function to split training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#splitting the training data into \n",
    "def create_training_sets (training_data):\n",
    "    # Create the features sets.  Call the function that was passed in.\n",
    "    # For names data, key is the name, and value is the gender\n",
    "\n",
    "    # Divided training and testing in thirds.  Could divide in other proportions instead.\n",
    "    third = int(float(len(training_data)) / 3.0)    \n",
    "    train_set, test_set = training_data[0:third*2], training_data[third*2:]\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Function to transform pandas for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful Transformer from http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "# This pulls a single column from a supplied pandas dataframe for classification.\n",
    "class ColumnExtractor(TransformerMixin):\n",
    "    def __init__(self, columns=[]):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X[self.columns]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Feature Building Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def otherFeature(sentence, retweet):\n",
    "    blob = TextBlob(sentence)\n",
    "    sentimentSum = 0\n",
    "    tweet = 0\n",
    "    for sentence in blob.sentences:\n",
    "        sentimentSum += sentence.sentiment.polarity\n",
    "    if retweet == None:\n",
    "        tweet =  0\n",
    "    else:\n",
    "        tweet = 1\n",
    "    count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "    a_punct = count(sentence, string.punctuation)\n",
    "    dict_ = {'sentiment' : sentimentSum,\n",
    "    'textLength' : len(sentence),\n",
    "    'isRetweet': tweet,\n",
    "    'numPunct': a_punct}\n",
    "    return dict_\n",
    "\n",
    "def sentimentScore(sentence):\n",
    "    blob = TextBlob(sentence)\n",
    "    sentimentSum = 0\n",
    "    for sentence in blob.sentences:\n",
    "        sentimentSum += sentence.sentiment.polarity\n",
    "    return sentimentSum\n",
    "\n",
    "def textLength(text):\n",
    "    return len(text)\n",
    "\n",
    "def isRetweet(text):\n",
    "    if text == None:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def numPunct(text):\n",
    "    count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "    a_punct = count(text, string.punctuation)\n",
    "    return a_punct\n",
    "\n",
    "def leskize(word_pos_str):\n",
    "    \"\"\"Returns the most probable WordNet lemmas for each term in a string by applying the lesk aglorithm\n",
    "    to each term, given its tagged POS and the string context\n",
    "    \n",
    "    Arguments:\n",
    "    word_pos_str -- str consisting of \"word POS word POS word POS ...\"\n",
    "    \n",
    "    Returns:\n",
    "    list\n",
    "    \"\"\"\n",
    "    wn_pos = {'VERB': 'v', 'NOUN': 'n', 'ADV': 'r', 'ADJ': 'a'}\n",
    "    \n",
    "    ss_list = []\n",
    "\n",
    "    word_pos_pairs = word_pos_str.split()\n",
    "    words = word_pos_pairs[::2]\n",
    "    for i in range(0, len(word_pos_pairs), 2):\n",
    "        if word_pos_pairs[i+1] in wn_pos:\n",
    "            ss = lesk(words, word_pos_pairs[i], wn_pos[word_pos_pairs[i+1]])\n",
    "            if not ss:\n",
    "                ss = lesk(words, word_pos_pairs[i])\n",
    "            if ss:\n",
    "                ss_list.append(ss)\n",
    "\n",
    "    return ss_list\n",
    "\n",
    "def build_features(df):\n",
    "    \"\"\"Do basic processing of the input text and generate features based on it in different columns\n",
    "\n",
    "    Arguments:\n",
    "    df -- DataFrame (the pandas dataframe with Category & Text columns already defined)\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "#     # Part of Speech in the form \"word POS word POS word POS ...\"\n",
    "#     df['pos'] = df.only_text_splithashtag.apply(lambda sent: ' '.join([' '.join([wnl.lemmatize(word), tag]) for word, tag\n",
    "#                                                               in pos_tag(wordpunct_tokenize(sent),\n",
    "#                                                                          tagset='universal') if\n",
    "#                                                               tag[0] not in string.punctuation]))\n",
    "\n",
    "#     # A list of most probable lemma synsets (not used directly, but useful)\n",
    "#     df['synsets'] = df.pos.apply(leskize)\n",
    "\n",
    "#     # The definitions of each word, concatenated\n",
    "#     df['definition'] = df.synsets.apply(lambda sss: ' '.join([ss.definition() for ss in sss]))\n",
    "\n",
    "    # q-grams (generated from Chris' abydos package, which performed a little better than TfidfVectorizer)\n",
    "    # These are also known as k-grams, shingles, k-mers, and (character-wise) n-grams\n",
    "    df['other_features'] = df.apply(lambda x: otherFeature(x['only_text_splithashtag'], x['retweet']), axis=1)\n",
    "    \n",
    "    \n",
    "    df['qgrams'] = df.only_text_splithashtag.apply(lambda s: dict(QGrams(s, 4, start_stop='') + QGrams(s, 5, start_stop='')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Pipeline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok = happyfuntokenizing.TweetTokenizer(preserve_case=False)\n",
    "# Logistic classifier, using WordNet-lemmatized unigrams & bigrams as features\n",
    "log_lem12_pipeline = Pipeline([\n",
    "            ('tfidf_lemmatized', Pipeline([\n",
    "                    ('extract', ColumnExtractor('tokenized')),\n",
    "                    ('vectorize', CountVectorizer(ngram_range=(1, 2),\n",
    "                                                  lowercase=True, tokenizer=tok.tokenize)),\n",
    "            ])),\n",
    "            ('classifier', LogisticRegression(random_state=seed()))])\n",
    "\n",
    "lsvc_lem12_pipeline = Pipeline([\n",
    "            ('tfidf_lemmatized', Pipeline([\n",
    "                    ('extract', ColumnExtractor('tokenized')),\n",
    "                    ('vectorize', TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True, norm='l2',\n",
    "                                                  lowercase=True, tokenizer=tok.tokenize)),\n",
    "                    # LSA is cool, in theory, but didn't work here, except with huge values & overfitting\n",
    "                    #('lsa', TruncatedSVD(n_components=2100, algorithm='arpack', random_state=seed())),\n",
    "            ])),\n",
    "            ('classifier', LinearSVC(loss='hinge', C=1, random_state=seed()))])\n",
    "\n",
    "# SVM with a linear kernel, using unigrams & bigrams of sentences passed through soundex and through\n",
    "# double metaphone as features\n",
    "lsvc_sdx_pipeline = Pipeline([\n",
    "            ('features', FeatureUnion([\n",
    "                ('definition', Pipeline([\n",
    "                    ('extract', ColumnExtractor('only_text_splithashtag')),\n",
    "                    ('vectorize', TfidfVectorizer(ngram_range=(1, 3), sublinear_tf=True, norm='l2',\n",
    "                                                  lowercase=False)),\n",
    "                ])),\n",
    "#                 ('definition', Pipeline([\n",
    "#                     ('extract', ColumnExtractor('only_text_splithashtag')),\n",
    "#                     ('vectorize', CountVectorizer(ngram_range=(1, 2),lowercase=True)),\n",
    "#                 ])),\n",
    "#                 ('definition', Pipeline([\n",
    "#                         ('extract', ColumnExtractor('qgrams')),\n",
    "#                         ('vectorize', DictVectorizer()),\n",
    "#                 ])),\n",
    "                ('definition', Pipeline([\n",
    "                        ('extract', ColumnExtractor('other_features')),\n",
    "                         ('vectorize', DictVectorizer()),\n",
    "                ])),\n",
    "                    ])),\n",
    "                ('classifier', XGBClassifier(max_depth=8,n_estimators=128,))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6. Building the Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>only_emoji</th>\n",
       "      <th>only_text_splithashtag</th>\n",
       "      <th>retweet</th>\n",
       "      <th>split_hashtag</th>\n",
       "      <th>text</th>\n",
       "      <th>is_emoji</th>\n",
       "      <th>other_features</th>\n",
       "      <th>qgrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>301718</th>\n",
       "      <td>[]</td>\n",
       "      <td>777 - we became hella closer so quick . You ba...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>777- we became hella closer so quick. You bae ...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'textLength': 53, 'numPunct': 3, 'isRetweet':...</td>\n",
       "      <td>{'r so': 1, 'You b': 1, ' we b': 1, 'ou b': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104653</th>\n",
       "      <td>[]</td>\n",
       "      <td>I really want chicken mcnuggets . this is not ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>I really want chicken mcnuggets. this is not g...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'textLength': 18, 'numPunct': 1, 'isRetweet':...</td>\n",
       "      <td>{'od .': 1, 'this ': 1, ' is n': 1, 'hicke': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35389</th>\n",
       "      <td>[]</td>\n",
       "      <td>What happens when Caltrain fails : 8 coworkers...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>What happens when Caltrain fails: 8 coworkers ...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'textLength': 91, 'numPunct': 2, 'isRetweet':...</td>\n",
       "      <td>{' Cal': 1, 'ns wh': 1, 'rill': 1, 'caron': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785592</th>\n",
       "      <td>[]</td>\n",
       "      <td>Just landed . Back to the 2nd home . ( @ San F...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>Just landed. Back to the 2nd home. (@ San Fran...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'textLength': 67, 'numPunct': 6, 'isRetweet':...</td>\n",
       "      <td>{'Fran': 1, ' hom': 1, ') ur': 1, ' . ( ': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733640</th>\n",
       "      <td>[]</td>\n",
       "      <td>BCRF @ Casa de Johnson url</td>\n",
       "      <td>None</td>\n",
       "      <td>[[BCRF]]</td>\n",
       "      <td>#BCRF @ Casa de Johnson  url</td>\n",
       "      <td>0</td>\n",
       "      <td>{'textLength': 26, 'numPunct': 1, 'isRetweet':...</td>\n",
       "      <td>{'hnso': 1, 'asa ': 1, ' de J': 1, 'son u': 1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       only_emoji                             only_text_splithashtag retweet  \\\n",
       "301718         []  777 - we became hella closer so quick . You ba...    None   \n",
       "104653         []  I really want chicken mcnuggets . this is not ...    None   \n",
       "35389          []  What happens when Caltrain fails : 8 coworkers...    None   \n",
       "785592         []  Just landed . Back to the 2nd home . ( @ San F...    None   \n",
       "733640         []                         BCRF @ Casa de Johnson url    None   \n",
       "\n",
       "       split_hashtag                                               text  \\\n",
       "301718            []  777- we became hella closer so quick. You bae ...   \n",
       "104653            []  I really want chicken mcnuggets. this is not g...   \n",
       "35389             []  What happens when Caltrain fails: 8 coworkers ...   \n",
       "785592            []  Just landed. Back to the 2nd home. (@ San Fran...   \n",
       "733640      [[BCRF]]                      #BCRF @ Casa de Johnson  url    \n",
       "\n",
       "        is_emoji                                     other_features  \\\n",
       "301718         0  {'textLength': 53, 'numPunct': 3, 'isRetweet':...   \n",
       "104653         0  {'textLength': 18, 'numPunct': 1, 'isRetweet':...   \n",
       "35389          0  {'textLength': 91, 'numPunct': 2, 'isRetweet':...   \n",
       "785592         0  {'textLength': 67, 'numPunct': 6, 'isRetweet':...   \n",
       "733640         0  {'textLength': 26, 'numPunct': 1, 'isRetweet':...   \n",
       "\n",
       "                                                   qgrams  \n",
       "301718  {'r so': 1, 'You b': 1, ' we b': 1, 'ou b': 1,...  \n",
       "104653  {'od .': 1, 'this ': 1, ' is n': 1, 'hicke': 1...  \n",
       "35389   {' Cal': 1, 'ns wh': 1, 'rill': 1, 'caron': 1,...  \n",
       "785592  {'Fran': 1, ' hom': 1, ') ur': 1, ' . ( ': 1, ...  \n",
       "733640  {'hnso': 1, 'asa ': 1, ' de J': 1, 'son u': 1,...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_random['is_emoji'] = tweet_random.text.apply(is_emoji)\n",
    "build_features(tweet_random)\n",
    "tweet_random.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_random.drop(['only_emoji', 'retweet'], axis=1,inplace=True)\n",
    "#tweet_random['tokenized'] = tweet_random.only_text_splithashtag.apply(tok.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##7. Splitting the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set, test_set = create_training_sets(tweet_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##8. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812303501572\n",
      "time to completion: 00:59:41\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "start_time = time.time()\n",
    "\n",
    "TESTING = False\n",
    "model = lsvc_sdx_pipeline.fit(train_set, train_set['is_emoji'])\n",
    "prediction = model.predict(test_set)\n",
    "score = accuracy_score(test_set['is_emoji'], prediction)\n",
    "print(score)\n",
    "\n",
    "sec = time.time() - start_time\n",
    "hours, remainder = divmod(sec, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print('time to completion: %02d:%02d:%02d' % (hours, minutes, seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9. Saving The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('xgboost_model.pickle', 'wb') as fin:\n",
    "    pickle.dump(model, fin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
