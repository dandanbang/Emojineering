{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning steps:\n",
    "- replace @ handles with: hdl (chose a word with no punctuation so no extra step is needed to process, hdl = handle)\n",
    "- replace urls by: url\n",
    "- replace emoticons with corresponding emojis\n",
    "- split retweets in tweet and retweet\n",
    "\n",
    "For all of these, first check what is the prevalence and if it is worth the effort. \n",
    "\n",
    "Ideas:\n",
    "- split hashtags into words -> Carlo tackling that\n",
    "- Tweets not in english -> Daniel tackling that\n",
    "- replace contractions & spellchecking\n",
    "    - Character ngram will probably be more efficient due to the really low quality of speach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 140\n",
    "import nltk\n",
    "import re\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholder cannot be called in this file before the all subfunctions are defined\n",
    "def clean(df):\n",
    "    \"\"\"Data cleaning steps:\n",
    "    - replace @ handles with: hdl (chose a word with no punctuation so no extra step is needed to process, hdl = handle)\n",
    "    - replace urls by: url\n",
    "    - replace emoticons with corresponding emojis\n",
    "    - split retweets in tweet and retweet\n",
    "    \n",
    "    To be added:\n",
    "    - remove non english tweets, done on the text only (not retweet). Needs to be applied after emoji split. \n",
    "    - hashtags splits\n",
    "    \n",
    "    Returns None (data cleaning in place)\n",
    "    \"\"\"\n",
    "    cleanHandle(df)\n",
    "    cleanURL(df)\n",
    "    convertEmoticon(df)\n",
    "    cleanRetweets(df)\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loader(filename):\n",
    "    \"\"\" Load tweets from filename. Resets the index. Returns the loaded data frame\"\"\"\n",
    "    with open(filename,'r') as f:\n",
    "        df = pd.DataFrame(json.load(f))\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace @ handles with hdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanHandle(df):\n",
    "    \"\"\" Replace in-place handles with hdl keyword\n",
    "    \n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    pattern = r\"@[a-zA-Z0-9_]{1,15}\" #from http://kagan.mactane.org/blog/2009/09/22/what-characters-are-allowed-in-twitter-usernames/\n",
    "    print(\"{} handles replaced\".format(np.sum(df.text.str.contains(pattern).values)))\n",
    "    df.text = df.text.str.replace(pattern, \" hdl \")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No \"hdl\" words for confusion in the txt. Good replacement name. \n",
    "\n",
    "Best to replace handles with \"\\ hdl\\ \", so for tokenization it will be easier to identify as a word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace URLs with url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanURL(df):\n",
    "    \"\"\" Replace in-place URLs with url keyword\n",
    "    \n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    pattern = r'(?:http://|https://|www.)[^‚Äú‚Äù\"\\' ]+' # From http://stackoverflow.com/questions/7679970/python-regular-expression-nltk-website-extraction\n",
    "    print(\"{} urls replaced\".format(np.sum(clean_tweets_df.text.str.contains(pattern).values)))\n",
    "    df.text = df.text.str.replace(pattern, \" url \")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URLs are quiet well formed and are generally at the end of tweets. No risk of engulfing in the cleaning some more text after the url.\n",
    "\n",
    "keyword url is used only 4 times in dataset, no risk of confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert emoticons to emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Based on:\n",
    "# https://slack.zendesk.com/hc/en-us/articles/202931348-Emoji-and-emoticons\n",
    "# http://unicodey.com/emoji-data/table.htm\n",
    "# http://www.unicode.org/emoji/charts/emoji-list.html\n",
    "\n",
    "def convertEmoticon(df):\n",
    "    \"\"\" Replace in-place common emoticons to emojis.\n",
    "    \n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    emoticon2emoji = {\n",
    "        r\"<3\": \"\\u2764\",\n",
    "        r\"</3\": \"\\U0001F494\",\n",
    "        r\"8\\)\": \"\\U0001F60E\",\n",
    "        r\"D:\": \"\\U0001F627\",\n",
    "        r\":'\\(\": \"\\U0001F622\",\n",
    "        r\":o\\)\": \"\\U0001F435\",\n",
    "        r\":-*\\*\": \"\\U0001F48B\",\n",
    "        r\"=-*\\)\": \"\\U0001F600\",\n",
    "        r\":-*D\": \"\\U0001F600\",\n",
    "        r\";-*\\)\": \"\\U0001F609\",\n",
    "        r\":-*>\": \"\\U0001F606\",\n",
    "        r\":-*\\|\": \"\\U0001F610\",\n",
    "        r\":-*[Oo]\": \"\\U0001F62E\",\n",
    "        r\">:-*\\(\": \"\\U0001F620\",\n",
    "        r\":-*\\)|\\(:\": \"\\U0001F603\",\n",
    "        r\":-*\\(|\\):\": \"\\U0001F61E\",\n",
    "        r\":-*[/\\\\]\": \"\\U0001F615\",\n",
    "        r\":-*[PpbB]\": \"\\U0001F61B\",\n",
    "        r\";-*[PpbB]\": \"\\U0001F61C\"\n",
    "    }\n",
    "    \n",
    "    total = 0\n",
    "    for emoticon in emoticon2emoji:\n",
    "        nreplacements = np.sum(df.text.str.contains(emoticon).values)\n",
    "        total += nreplacements\n",
    "        print(\"{:10} -> {:>5} replaced {:6} times\".format(emoticon, emoticon2emoji[emoticon], nreplacements))\n",
    "        df.text = df.text.str.replace(emoticon, emoticon2emoji[emoticon])\n",
    "    print(\"{:3} replaced {} times\".format(\"ALL\", total))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split retweets into user content and retweeted content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cannot be called before functions within are defined \n",
    "def cleanRetweets(df):\n",
    "    \"\"\" Remove from the text column the retweet column and add to seperate column \"retweet\". \n",
    "    \n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    nsplits = 0\n",
    "    nsplits += splitRetweets(df)\n",
    "    nsplits += splitQuotes(df) # Need to maintain that order\n",
    "    print(\"{} retweets processed\".format(nsplits))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitRetweets(df):\n",
    "    \"\"\" Extract retweets with the RT keyword\"\"\"\n",
    "    pattern = r\"\"\"(.*(?:\\W|^))(RT(?:\\ ?[\\\":‚Äú]|$).*) # Retweets with RT keyword\"\"\"\n",
    "    retweets = pd.DataFrame(df.text.str.extract(pattern, flags=re.X))\n",
    "    retweets.columns = [\"text\", \"retweet\"]\n",
    "    non_null_idxs = retweets.retweet.notnull()\n",
    "    df.loc[non_null_idxs,[\"text\"]] = retweets.text[non_null_idxs]\n",
    "    df[\"retweet\"] = retweets.retweet\n",
    "    return len(non_null_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def splitQuotes(df):\n",
    "    \"\"\" Extract retweets in quote format.\n",
    "    See http://support.gnip.com/articles/identifying-and-understanding-retweets.html\n",
    "    \"\"\"\n",
    "    pattern = r\"(.*?)((?:([\\\"\\'])|(?:(‚Äú))|‚Äò)\\s*hdl.*(?(3)\\3|(?(4)‚Äù|‚Äô)))(.*)\" #Pattern to match quote, possibly nested\n",
    "    retweets = pd.DataFrame(df.text.str.extract(pattern, flags=re.X)[[0,1,4]])\n",
    "    retweets.columns = [\"text_before\", \"retweet\", \"text_after\"]\n",
    "    non_null_idxs = retweets.retweet.notnull()\n",
    "    retweets[\"text\"] = retweets.loc[non_null_idxs, [\"text_before\", \"text_after\"]].apply(lambda x: \" \".join(x), axis=1)\n",
    "    df.loc[non_null_idxs,[\"text\"]] = retweets.text[non_null_idxs].copy()\n",
    "    df[\"retweet\"] = retweets.retweet.copy()\n",
    "    return len(retweets[non_null_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save clean file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    clean_tweets_df = loader('./data/tweets_training.json')\n",
    "    clean(clean_tweets_df)\n",
    "    clean_tweets_df.to_json('./data/tweets_training_clean.json', force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Functions to extract only emoji or only text from input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textEmojiOnly(df):\n",
    "    \"\"\" Function to Create Two New Columns [Text Only] & [Emoji Only]\"\"\"\n",
    "    df['Emoji'] = [emojiExtract(word) for word in df.text]\n",
    "    df['only_Text'] = [textExtract(word) for word in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Wide UCS-4 build\n",
    "    highpoints = re.compile(u'['\n",
    "        u'\\U0001F300-\\U0001F64F'\n",
    "        u'\\U0001F680-\\U0001F6FF'\n",
    "        u'\\u2600-\\u26FF\\u2700-\\u27BF]+', \n",
    "        re.UNICODE)\n",
    "except re.error:\n",
    "    # Narrow UCS-2 build\n",
    "    highpoints = re.compile(u'('\n",
    "        u'\\ud83c[\\udf00-\\udfff]|'\n",
    "        u'\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|'\n",
    "        u'[\\u2600-\\u26FF\\u2700-\\u27BF])+', \n",
    "        re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to check whether there's an emoji in the text, return 1 if true, 0 if false\n",
    "def is_emoji(text):\n",
    "    if highpoints.search(text):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emojiExtract(sent):\n",
    "    return [word for word in tok.tokenize(sent) if is_emoji(word) == 1]\n",
    "\n",
    "def textExtract(sent):\n",
    "    return [word for word in tok.tokenize(sent) if is_emoji(word) == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Twitter hashtags\n",
    "To be completed by Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Functions to clean non-english columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = string.punctuation\n",
    "ex = ['‚Äú', '‚Äî', '‚Äô', ' Ô∏è', 'Ô∏è', '...', '‚Äù', '‚Ä¶', ' Óêí, Óêí, Óêí,', '?ÓêÇ', ' ÓêÖ', ' ‚É£', '‚àû', 'üÜí']\n",
    "for pun in [word for word in ex if word not in punctuation]:\n",
    "    punctuation += pun\n",
    "def isEnglish(list):\n",
    "    try:\n",
    "        [word.encode('ascii') for word in list if word not in punctuation]\n",
    "    except Exception:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanNonEnglish(df):\n",
    "    \"\"\" \n",
    "    \n",
    "    Needs to be applied after emoji splitting as emojis are considered non-english\n",
    "    \"\"\"\n",
    "    text_list = df['text'].values\n",
    "    english_Boolean = [isEnglish(sent) for sent in text_list]\n",
    "    return df[english_Boolean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test = cleanNonEnglish(clean_tweets_df.iloc[:10000])\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
